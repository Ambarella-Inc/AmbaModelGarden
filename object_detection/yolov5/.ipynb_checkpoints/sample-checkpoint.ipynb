{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AmbaMPS using SageMaker APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a sample jupyter notebook that explains how to use Ambarella's Model Preperation Service (AmbaMPS) using AWS SageMaker Python APIs. \n",
    "Users can subscribe to the algorithm from the product listing page: \n",
    "[Ambarella MPS](https://aws.amazon.com/marketplace/pp/prodview-zf6dvvlikubbu)\n",
    "\n",
    "The algorithm published does Object Detection using Yolov5 on user's dataset. There are 2 components to SageMaker: \n",
    "\n",
    "**1. Preparing the model:**<br> \n",
    "Users are provided with options for hyper-parameters allowing them to train the model for their dataset. The output of training is Ambarella's proprietary AmbaPB fast checkpoint best suited for running optimized inference on Ambarella's CV chips. \n",
    "\n",
    "**2. Running Inference:**<br> \n",
    "Users can run batch transforms or real time predictions on the output of training. The inference does the detection and outputs in application/json format which contains the boxes, scores and labels output.\n",
    "\n",
    "The notebook shows example using coco128 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "<br>\n",
    "\n",
    "**1. Configure AWS**<br> \n",
    "Please install AWS CLI and run `aws configure` to configure your aws account.\n",
    "\n",
    "<br>\n",
    "\n",
    "**2. CVTOOLS**<br> \n",
    "The inference sections of the demo uses Ambarella's CVTOOLS. Please contact Ambarella for CVTOOLS access\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is based off of Ultralytics/yolov5 Git repo. Users are adviced to prepare the dataset in the format required by Ultralytics.[Training Custom Data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)\n",
    "\n",
    "Here is a sample format to prepare the dataset.yaml file:\n",
    "\n",
    "\n",
    "`path: coco128` <br>\n",
    "`train: images/train2017  # train images (relative to 'path')` <br>\n",
    "`val: images/val2017  # val images (relative to 'path')` <br>\n",
    "\n",
    "`# Classes` <br>\n",
    "`names:` <br>\n",
    "`  0: person` <br> \n",
    "`  1: bicycle` <br>\n",
    "`  2: car` <br>\n",
    "`  ...` <br>\n",
    "`  77: teddy bear` <br>\n",
    "`  78: hair drier` <br>\n",
    "`  79: toothbrush` <br>\n",
    "\n",
    "\n",
    "The dataset.yaml must be placed at the same root level as the dataset folder. The name of the dataset folder must be the same name as the value of the path. For example: \n",
    "\n",
    "* Example 1 <br>\n",
    "`VOC.yaml`<br>\n",
    "`VOC/`<br>\n",
    "<br>\n",
    "* Example 2 <br>\n",
    "`coco128.yaml` <br>\n",
    "`coco128/` <br>\n",
    "<br>\n",
    "* Example 3 <br>\n",
    "`VisDrone.yaml` <br>\n",
    "`VisDrone/` <br>\n",
    "<br>\n",
    "\n",
    "**The example in the notebook is based of COCO128 dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training requires the following: \n",
    "1. SageMaker python package installed.<br>\n",
    "2. Hyper-parameters as dictionary<br>\n",
    "3. Instance type<br>\n",
    "4. Subscription to Ambarella's algorithm<br>\n",
    "5. Access to S3 location for input and output data exchanges<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install sagemaker python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "import boto3\n",
    "import json\n",
    "import enum\n",
    "import tarfile\n",
    "from IPython.display import Image as display_image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from misc.utils import set_cvb_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \n",
    "    \"model_type\":\"small\",\n",
    "    \"total_epochs\":\"100\",\n",
    "    \"batch_size\":\"32\",\n",
    "    \"workers\":\"8\",\n",
    "    \"optimizer\":\"SGD\",\n",
    "    \"image_size\":\"640\",\n",
    "    \"initial_lr\":\"0.01\",\n",
    "    \"final_lr\":\"0.0001\",\n",
    "    \"momentum\":\"0.9\",\n",
    "    \"weight_decay\":\"0.0005\",\n",
    "    \"warmup_epochs\":\"3\",\n",
    "    \"conf_thresh\":\"0.25\",\n",
    "    \"iou_thresh\":\"0.6\",\n",
    "    \"performance\":\"high_accuracy\",\n",
    "    \"patience\":\"10\",\n",
    "    \"max_detections\": \"300\",\n",
    "    \"chipset\": \"amba_cv22\"\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providing path to S3 location for train & val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to training and validation dataset \n",
    "input_data = {\n",
    "            'dataset':'s3://ambarella-tlt/dataset/data_coco128/'\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the estimator API and create an estimator object\n",
    "\n",
    "Create a role on AWS IAM to allow access to SageMaker<br>\n",
    "Provide a GPU instance such as ml.p2.xlarge to run training.<br>\n",
    "Provide the image_uri to the subscribed algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the estimator API with role, image_uri, hyper-parameters, s3 path.\n",
    "\n",
    "estimator = Estimator(\n",
    "    role='arn:aws:iam::612535784962:role/SageMakerTraining',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    image_uri='612535784962.dkr.ecr.us-west-2.amazonaws.com/ambamps_yolov5:latest',\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path='s3://ambarella-tlt/yolov5_out/',\n",
    "    base_job_name='test-notebook'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Call the fit() API to start training on AWS instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training log can be monitored here in the console or SageMaker console. <br>\n",
    "Instance and algorithm metrics can be monitored on the SageMaker console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](misc/sagemaker.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference involves creating an endpoint and sending image/jpg requests which runs forward pass on the model artifacts and streams back application/json response<br>\n",
    "\n",
    "There are 2 ways of running inference. \n",
    "\n",
    "1. Using the SageMaker APIs\n",
    "2. Using CVFlow APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the image to run inference on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a few image samples\n",
    "class Images(enum.Enum):\n",
    "    img_1 = 'images/000000000030.jpg'\n",
    "    img_2 = 'images/000000000089.jpg'\n",
    "    img_3 = 'images/000000000081.jpg'\n",
    "    \n",
    "image_file = Images.img_2.value\n",
    "display_image(image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Running Inference using SageMaker APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample code below shows how to use sagemaker's runtime API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Instantiate SageMaker runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = boto3.Session().client(service_name='sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Deploy & Get endpoint name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is required to deploy the model and get the endpoint name of the model <br>\n",
    "There are 2 options: \n",
    "\n",
    "1. Use the deploy python API and get the endpoint name\n",
    "2. On the SageMaker console, create a Model with endpoint configuration under the Inference section\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Using deploy API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploys the model that was generated by fit() to a SageMaker endpoint\n",
    "deploy = estimator.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge')\n",
    "endpoint_name = deploy.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Using SageMaker console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model under inference/endpoint section on the SageMaker console\n",
    "endpoint_name = 'test-notebook'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Load the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = open(image_file, 'rb').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Call the runtime API and read the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='image/jpg', \n",
    "    Body=bytearray(img)\n",
    ")\n",
    "\n",
    "# Read the `Body` of the response (in application/json format)\n",
    "result = response['Body'].read()\n",
    "\n",
    "# Load the result into a dictionary\n",
    "res_dict = json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = json.dumps(res_dict, indent=4)\n",
    "with open(\"sample_output.json\", 'w') as f:\n",
    "    f.write(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Get the output and display the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocess.py is a cli tool that takes in model output and plots boxes on the image.\n",
    "# Usage: python3 path/to/postprocess.py --data path/to/data.yaml --source image_file.jpg --detection-output path/to/detection_output_from_host_or_board\n",
    "\n",
    "# The below example shows how to import and use the run() method\n",
    "# The output images will be in the `postprocess/` folder.\n",
    "\n",
    "from scripts import postprocess\n",
    "\n",
    "output = np.asarray(res_dict['output'][0], dtype=np.float32)\n",
    "\n",
    "out_image = postprocess.run(detection_output=output,\n",
    "                source=image_file,\n",
    "                imgsz=(int(hyperparameters['image_size']), int(hyperparameters['image_size'])),\n",
    "                data='data/coco128.yaml'\n",
    "               )\n",
    "display_image(out_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Running Inference on the Host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section requires users to have access to Ambarella's CVTOOLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cvflowbackend library\n",
    "set_cvb_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Retrieve the model from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'ambarella-tlt'\n",
    "KEY = 'yolov5_out/test-notebook-2022-12-15-18-43-12-373/output/model.tar.gz'\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(BUCKET_NAME).download_file(KEY, 'model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tarfile.open('model.tar.gz')\n",
    "output.extractall()\n",
    "ambapb_checkpoint = \"output/yolov5s.ambapb.ckpt.onnx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](misc/AmbaPB.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above image represents AmbaPB which Ambarella's proprietary model representation format. The format extends the ONNX IR specification to express CVflow computational primitives and store artifacts produced by accompanying CVTools.\n",
    "\n",
    "The image represents a sample AmbaPB checkpoint model that has 2 inputs: `images_y` and `images_uv` and a single output named `output`\n",
    "\n",
    "The shapes are represented as NCHW format. The above AmbaPB file contains heterogenous graph that runs the YOLOv5 neural network on Ambarella's Vector Processor (VP) and the NMS on ARM node called ACA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Running Validation on the ambapb model to get mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_amba.py is a cli tool that generates mAP of the final trained ambapb model.\n",
    "# Usage: python3 path/to/val_amba.py --weights path/to/yolov5s.ambapb.ckpt.onnx --data path/to/data.yaml\n",
    "# Note: Use Batch Size parameter as 1.\n",
    "\n",
    "# The below example shows how to import and use the run() method\n",
    "\n",
    "from scripts import val_amba\n",
    "\n",
    "mAP = val_amba.run(\n",
    "    data='data/coco128.yaml',\n",
    "    weights=ambapb_checkpoint\n",
    ")\n",
    "print(\"Trained model mAP: {}\".format(mAP[0][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Running the detect and plot images on the ambapb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect.py is a cli tool that runs inference on the ambapb model and plots the bounding box\n",
    "# Usage: python3 path/to/detect.py --weights path/to/yolov5s.ambapb.ckpt.onnx --data path/to/data.yaml --source single_img/directory_of_imgs/\n",
    "\n",
    "# The below example shows how to import and use the run() method\n",
    "# The output images will be in the `detections/out/` folder.\n",
    "\n",
    "from scripts import detect\n",
    "\n",
    "detect.run(\n",
    "    data='data/coco128.yaml',\n",
    "    weights=ambapb_checkpoint,\n",
    "    source='images/',\n",
    "    project='detections',\n",
    "    name='out'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Advanced Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running inference on the host in a pythonic way is possible with Ambarella's CVTOOLS. With this, users will have access to the numpy arrays and this can be plugged into other python scripts. \n",
    "\n",
    "Python API based Inference involves the following: \n",
    "\n",
    "1. Create an Inference Session \n",
    "2. Create a feed dictionary for the inputs\n",
    "3. Call sess.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "import cvflowbackend\n",
    "from cvflowbackend.evaluation_stage import InferenceSession\n",
    "from cvflowbackend.evaluation_stage.session import TensorFileReader as TFR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Create an Inference session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference session\n",
    "sess = InferenceSession(ambapb_checkpoint, mode=\"acinf\", cuda_devices=0)\n",
    "i_configs = sess.get_input_configs()\n",
    "o_configs = sess.get_output_configs()\n",
    "\n",
    "print(\"Displaying the input configuration of the AmbaPB: {}\".format(i_configs))\n",
    "print(\"\\nDisplaying the output configuration of the AmbaPB: {}\".format(o_configs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Call the preprocess method to generate Y and UV binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess.py is a cli tool that generates Y and UV interleaved binaries from JPG files. \n",
    "# Usage: python3 path/to/preprocess.py ---source image_file/folder --imgsz 640\n",
    "\n",
    "# The below example shows how to import and use the run() method\n",
    "# The output images will be in the `preprocess/` folder.\n",
    "\n",
    "from scripts import preprocess\n",
    "\n",
    "# The preprocess script returns path to the .txt file containing the binaries\n",
    "y_bin_file, uv_bin_file = preprocess.run(\n",
    "                            source=image_file,\n",
    "                            imgsz=(int(hyperparameters['image_size']), int(hyperparameters['image_size']))\n",
    "                          )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Create a feed dict for an input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {}\n",
    "\n",
    "# Create a TensorFileReader object for Y input\n",
    "y_tensor = TFR(t=sess.get_inputs()[0].name, \n",
    "                src=y_bin_file)\n",
    "\n",
    "# Create a TensorFileReader object for UV input with dram_format set to 1. Ambarella's sensors output UV interleaved as UVUV.\n",
    "uv_tensor = TFR(t=sess.get_inputs()[1].name, \n",
    "                src=uv_bin_file,\n",
    "                dram_format=1)\n",
    "\n",
    "\n",
    "feed_dict[sess.get_inputs()[0].name] = y_tensor\n",
    "feed_dict[sess.get_inputs()[1].name] = uv_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Set inputs and run forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inputs\n",
    "sess.set_inputs(feed_dict)\n",
    "\n",
    "# Run forward pass\n",
    "res_dict = sess.run(fwd_quantized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Get the output and display the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.asarray(res_dict['output'][0], dtype=np.float32)\n",
    "\n",
    "out_image = postprocess.run(detection_output=output,\n",
    "                source=image_file,\n",
    "                imgsz=(int(hyperparameters['image_size']), int(hyperparameters['image_size'])),\n",
    "                data='data/coco128.yaml'\n",
    "               )\n",
    "display_image(out_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
